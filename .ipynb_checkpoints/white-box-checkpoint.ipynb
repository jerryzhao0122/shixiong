{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import collections\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.2+cu113'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        self.dropout = nn.Dropout(p=0.5)  \n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(self.dropout(x)))\n",
    "        x = F.relu(self.fc2(self.dropout(x)))\n",
    "        x = self.fc3(self.dropout(x))\n",
    "        return F.log_softmax(x, dim=1)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model = CNN().cuda()\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "            root=\"/mnt/data/cifar10\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=ToTensor()\n",
    "        )\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "samp_idxs = list(range(30000))\n",
    "train_ld = DataLoader(DatasetSplit(train_dataset,samp_idxs),batch_size=512,shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.02,momentum=0.5)\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "epoch = 300\n",
    "for e in tqdm(range(epoch)):\n",
    "    for batch_idx,(data,target) in enumerate(train_ld):\n",
    "        data,target = data.cuda(),target.cuda()\n",
    "        log_probs = model(data)\n",
    "        loss = criterion(log_probs,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if e in [100,150,200,250,300]:\n",
    "        path = '/mnt/models/white_box_target_test'+str(e)+'.pth.tar'\n",
    "        torch.save(model.state_dict(),path)\n",
    "        print('已经保存')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [6, 3, 5, 5], expected input[2, 6, 14, 14] to have 3 channels, but got 6 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/FedL/white-box.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B39.106.29.208/home/FedL/white-box.ipynb#ch0000005vscode-remote?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m CNN()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B39.106.29.208/home/FedL/white-box.ipynb#ch0000005vscode-remote?line=1'>2</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m32\u001b[39m,\u001b[39m32\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B39.106.29.208/home/FedL/white-box.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/FedL/white-box.ipynb Cell 5'\u001b[0m in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B39.106.29.208/home/FedL/white-box.ipynb#ch0000004vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B39.106.29.208/home/FedL/white-box.ipynb#ch0000004vscode-remote?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B39.106.29.208/home/FedL/white-box.ipynb#ch0000004vscode-remote?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B39.106.29.208/home/FedL/white-box.ipynb#ch0000004vscode-remote?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B39.106.29.208/home/FedL/white-box.ipynb#ch0000004vscode-remote?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m16\u001b[39m \u001b[39m*\u001b[39m \u001b[39m5\u001b[39m \u001b[39m*\u001b[39m \u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=444'>445</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=445'>446</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=437'>438</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=438'>439</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=439'>440</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=440'>441</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=441'>442</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    <a href='file:///root/anaconda3/envs/yuan/lib/python3.9/site-packages/torch/nn/modules/conv.py?line=442'>443</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [6, 3, 5, 5], expected input[2, 6, 14, 14] to have 3 channels, but got 6 channels instead"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.4042e-01, 2.1662e-01, 3.6846e-01, 5.5762e-01, 6.9831e-01,\n",
       "          2.5403e-01, 9.1043e-01, 1.3228e-01, 9.5250e-01, 5.9735e-01,\n",
       "          2.6775e-01, 7.9251e-01, 3.4502e-01, 7.5535e-01, 6.0170e-01,\n",
       "          4.9258e-01, 3.5666e-01, 6.0216e-01, 5.6008e-01, 2.4180e-01,\n",
       "          8.9289e-01, 7.4972e-01, 5.3554e-01, 4.8578e-02, 7.7872e-01,\n",
       "          2.5680e-01, 5.3897e-01, 8.5064e-01],\n",
       "         [3.4261e-01, 3.0556e-01, 5.8713e-01, 5.2387e-02, 8.4311e-01,\n",
       "          1.0012e-01, 4.4809e-01, 9.6678e-01, 8.3146e-01, 7.2051e-01,\n",
       "          4.0973e-01, 1.5602e-01, 8.6390e-01, 5.8205e-01, 9.2642e-01,\n",
       "          2.1494e-01, 2.5396e-01, 2.3985e-04, 2.5392e-01, 1.7236e-01,\n",
       "          6.9787e-01, 3.5010e-01, 4.3254e-01, 6.1076e-01, 9.7910e-01,\n",
       "          8.2272e-01, 3.8809e-01, 4.5408e-01],\n",
       "         [4.1474e-01, 3.7963e-01, 8.4721e-02, 8.4245e-01, 6.0582e-01,\n",
       "          5.9843e-01, 1.5060e-01, 2.1752e-01, 5.8557e-01, 9.6576e-01,\n",
       "          6.0635e-01, 8.1413e-01, 8.7421e-01, 3.2778e-01, 9.1679e-01,\n",
       "          5.7307e-01, 3.3167e-01, 8.5885e-01, 3.6566e-01, 2.4907e-01,\n",
       "          6.1318e-01, 5.7458e-01, 4.4291e-02, 8.3360e-01, 1.2103e-01,\n",
       "          2.8640e-02, 5.2828e-01, 8.1790e-01],\n",
       "         [4.1080e-01, 7.5400e-01, 6.2982e-01, 5.5777e-02, 3.2496e-01,\n",
       "          5.9177e-01, 4.0298e-02, 2.3577e-01, 9.2468e-01, 1.5184e-01,\n",
       "          1.6194e-01, 8.5643e-01, 3.2495e-01, 9.3385e-01, 7.2127e-01,\n",
       "          6.0944e-01, 8.2693e-01, 4.5539e-01, 8.0110e-01, 6.8790e-01,\n",
       "          4.6802e-01, 2.0794e-01, 2.6523e-01, 7.0565e-01, 7.2293e-01,\n",
       "          9.7716e-01, 3.0610e-02, 4.7054e-01],\n",
       "         [1.0532e-01, 8.7551e-01, 7.3640e-01, 7.3674e-01, 8.7672e-01,\n",
       "          5.7850e-01, 1.8713e-01, 9.4595e-01, 1.8594e-01, 4.7613e-02,\n",
       "          1.3175e-01, 4.1107e-01, 2.0271e-01, 2.0798e-01, 2.5996e-02,\n",
       "          1.6098e-02, 2.9637e-01, 5.3345e-01, 7.2129e-02, 2.2748e-01,\n",
       "          5.1635e-01, 9.9794e-01, 5.9318e-01, 7.3059e-01, 8.5134e-01,\n",
       "          4.4410e-01, 6.5732e-01, 8.9836e-01],\n",
       "         [2.5305e-01, 2.5497e-02, 2.4811e-01, 8.8314e-01, 3.2117e-01,\n",
       "          8.8893e-01, 5.3276e-01, 9.4276e-01, 7.8516e-01, 6.7705e-01,\n",
       "          2.3085e-01, 7.6159e-01, 7.3659e-01, 8.1296e-01, 5.3028e-01,\n",
       "          7.2578e-01, 2.9495e-01, 6.8308e-01, 2.6021e-01, 4.5190e-02,\n",
       "          2.7638e-01, 8.0665e-01, 3.9180e-01, 9.1578e-01, 3.9223e-01,\n",
       "          7.0144e-01, 1.7471e-01, 4.5374e-01],\n",
       "         [4.3756e-01, 7.0681e-01, 9.7502e-01, 1.3053e-02, 5.0349e-01,\n",
       "          6.4315e-01, 1.5699e-01, 6.8810e-01, 3.3160e-01, 6.4121e-01,\n",
       "          6.4141e-01, 8.3715e-01, 8.6168e-01, 8.4646e-01, 5.5802e-01,\n",
       "          2.0668e-02, 1.2518e-01, 7.9261e-01, 3.5417e-01, 7.9476e-01,\n",
       "          4.3338e-01, 1.6757e-01, 5.9119e-02, 1.2424e-01, 3.7852e-01,\n",
       "          5.7864e-01, 8.9149e-01, 7.7580e-01],\n",
       "         [9.5595e-01, 3.7369e-01, 6.9496e-01, 6.9463e-01, 8.2793e-01,\n",
       "          9.1252e-01, 8.9942e-01, 6.7123e-01, 6.8544e-02, 9.2540e-01,\n",
       "          2.8254e-02, 9.6964e-01, 6.9730e-01, 6.5793e-01, 4.8052e-01,\n",
       "          9.9434e-01, 7.7203e-01, 4.5248e-01, 7.9336e-01, 1.6426e-01,\n",
       "          4.9526e-01, 3.3546e-01, 3.4210e-01, 5.2924e-01, 5.2937e-01,\n",
       "          8.5113e-01, 4.4579e-02, 8.7200e-01],\n",
       "         [8.9208e-01, 1.8338e-01, 5.5071e-01, 8.1286e-01, 4.3992e-01,\n",
       "          1.8741e-01, 3.3759e-01, 1.3569e-01, 1.7424e-01, 8.4101e-01,\n",
       "          6.6139e-01, 9.4306e-01, 6.6180e-01, 7.9020e-01, 3.2067e-02,\n",
       "          1.0239e-01, 3.7999e-03, 6.3583e-01, 9.1444e-01, 3.6066e-01,\n",
       "          5.5647e-01, 5.2959e-01, 1.0074e-01, 1.9120e-01, 5.3381e-01,\n",
       "          5.8817e-01, 1.9044e-01, 9.2340e-01],\n",
       "         [1.5523e-01, 8.3076e-01, 5.6095e-01, 5.2363e-01, 7.8837e-01,\n",
       "          1.0665e-01, 9.8107e-01, 7.8881e-01, 6.5185e-01, 4.5144e-01,\n",
       "          7.8071e-01, 8.3580e-01, 1.0771e-02, 5.0162e-01, 5.1219e-01,\n",
       "          1.3903e-01, 7.0093e-01, 4.4134e-01, 4.6391e-01, 9.9305e-01,\n",
       "          4.7344e-01, 3.9555e-01, 7.8835e-01, 2.9169e-01, 6.1323e-01,\n",
       "          8.5793e-01, 6.5278e-01, 4.8481e-01],\n",
       "         [7.9119e-01, 6.1827e-01, 1.7926e-01, 8.1111e-01, 5.7433e-01,\n",
       "          4.7020e-01, 4.8949e-01, 2.0227e-01, 7.1440e-01, 7.7902e-01,\n",
       "          6.5293e-01, 7.1370e-01, 7.5701e-01, 6.4923e-01, 4.2443e-01,\n",
       "          1.5933e-01, 5.9560e-02, 5.3989e-01, 2.9685e-01, 9.1971e-01,\n",
       "          1.2657e-01, 2.9781e-01, 1.8290e-01, 1.7744e-02, 7.7140e-01,\n",
       "          4.4842e-01, 4.1672e-01, 5.0676e-01],\n",
       "         [6.4097e-01, 8.9543e-01, 2.0718e-01, 3.7055e-01, 2.6641e-01,\n",
       "          4.3467e-01, 7.9849e-01, 8.7498e-01, 3.2205e-01, 5.6881e-01,\n",
       "          2.8077e-01, 8.0399e-01, 6.6828e-01, 1.8329e-01, 4.8776e-02,\n",
       "          8.2886e-01, 2.7227e-02, 6.9912e-01, 4.6867e-02, 2.2050e-01,\n",
       "          9.7622e-01, 3.0511e-01, 5.6691e-01, 8.2405e-01, 1.0301e-01,\n",
       "          5.0367e-01, 8.1377e-01, 9.3598e-02],\n",
       "         [7.8246e-01, 5.3051e-01, 5.6399e-01, 3.6282e-01, 1.2967e-01,\n",
       "          3.7895e-01, 9.7256e-01, 9.5918e-01, 3.3624e-01, 8.5805e-01,\n",
       "          4.3517e-01, 4.1380e-01, 9.4632e-01, 4.4365e-01, 4.4555e-01,\n",
       "          4.3327e-01, 1.6933e-01, 3.0016e-01, 7.8429e-01, 2.4623e-01,\n",
       "          1.6572e-02, 2.6743e-01, 1.2794e-01, 1.9316e-01, 2.9983e-01,\n",
       "          4.5476e-01, 3.9676e-01, 4.0092e-01],\n",
       "         [4.3990e-01, 8.0628e-01, 8.2370e-02, 5.2346e-02, 7.6891e-01,\n",
       "          9.7987e-01, 5.7068e-01, 2.0393e-01, 8.0443e-01, 1.2107e-01,\n",
       "          8.5986e-01, 6.0404e-01, 5.7350e-01, 4.2846e-01, 9.4114e-01,\n",
       "          2.6307e-01, 1.7462e-01, 3.3063e-01, 1.5421e-01, 4.3660e-01,\n",
       "          4.5410e-01, 7.4852e-01, 3.5055e-01, 7.3475e-04, 2.5358e-01,\n",
       "          1.9239e-01, 4.3617e-01, 1.3827e-01],\n",
       "         [1.1334e-01, 2.3739e-01, 7.6642e-01, 5.5117e-01, 3.6108e-01,\n",
       "          5.3649e-01, 9.7181e-01, 5.9700e-01, 8.6223e-01, 2.5610e-01,\n",
       "          9.7084e-01, 4.0346e-01, 3.9766e-01, 9.4104e-01, 1.6231e-01,\n",
       "          1.5603e-01, 1.3642e-01, 8.6855e-01, 7.6930e-01, 1.1022e-01,\n",
       "          9.9105e-01, 4.5004e-01, 3.8121e-01, 7.5269e-01, 9.2055e-01,\n",
       "          6.8794e-01, 1.3570e-01, 7.4623e-01],\n",
       "         [1.6692e-01, 8.5210e-01, 8.7676e-01, 5.6372e-01, 8.3619e-01,\n",
       "          7.9508e-01, 8.3568e-02, 5.6930e-01, 6.2778e-01, 6.7102e-01,\n",
       "          4.1788e-01, 9.9044e-01, 4.8581e-01, 3.9538e-01, 4.9145e-02,\n",
       "          4.0689e-02, 3.8746e-01, 7.3070e-01, 9.2309e-01, 8.6792e-01,\n",
       "          2.9637e-01, 2.5630e-01, 2.7762e-01, 6.0952e-01, 8.3711e-01,\n",
       "          6.7579e-01, 9.5547e-01, 4.0526e-01],\n",
       "         [5.1640e-01, 5.8042e-01, 1.4681e-01, 6.8485e-01, 9.6747e-01,\n",
       "          9.3375e-01, 5.2690e-01, 2.6800e-01, 9.9100e-01, 3.7119e-01,\n",
       "          6.5607e-01, 2.5974e-01, 2.2897e-01, 3.6827e-01, 7.3384e-01,\n",
       "          6.0075e-01, 3.9569e-01, 8.1563e-01, 2.1339e-01, 4.5277e-01,\n",
       "          4.5707e-01, 1.2125e-02, 3.4750e-01, 5.3098e-01, 4.2390e-01,\n",
       "          2.3277e-01, 4.1631e-01, 1.2981e-01],\n",
       "         [4.2537e-01, 9.8009e-01, 1.1419e-02, 7.1491e-01, 5.2918e-01,\n",
       "          4.2245e-01, 1.1237e-01, 4.3530e-01, 6.3461e-01, 3.9118e-01,\n",
       "          6.3389e-01, 5.4588e-01, 4.4666e-01, 5.7512e-01, 8.4831e-01,\n",
       "          7.1110e-01, 3.7200e-02, 5.7649e-01, 1.9411e-01, 8.3301e-01,\n",
       "          1.7010e-01, 6.6837e-01, 5.0177e-01, 1.9286e-01, 1.4731e-01,\n",
       "          1.3548e-01, 6.4179e-01, 6.5091e-01],\n",
       "         [3.0202e-01, 7.1631e-02, 6.5284e-01, 6.5295e-01, 9.2946e-01,\n",
       "          8.7541e-01, 5.7156e-01, 6.1286e-02, 6.1741e-01, 3.8366e-01,\n",
       "          1.5500e-01, 3.1635e-01, 9.8635e-01, 6.5823e-01, 1.5501e-01,\n",
       "          2.6632e-01, 5.9525e-01, 7.9439e-01, 7.5493e-01, 7.5984e-01,\n",
       "          6.6533e-02, 2.9978e-01, 8.0898e-01, 2.9915e-01, 8.5327e-01,\n",
       "          6.4709e-01, 3.6971e-01, 2.2251e-01],\n",
       "         [3.2771e-02, 3.2811e-01, 8.8457e-01, 2.4253e-01, 3.9047e-01,\n",
       "          3.8087e-01, 7.0663e-01, 6.2171e-01, 1.3599e-01, 9.6566e-01,\n",
       "          3.6388e-01, 4.7716e-01, 4.0915e-01, 3.8668e-01, 5.6839e-01,\n",
       "          6.1594e-01, 6.7730e-01, 4.0223e-01, 3.4624e-01, 2.1309e-01,\n",
       "          3.9789e-02, 1.9889e-01, 5.4186e-01, 3.7782e-01, 2.4447e-02,\n",
       "          6.7433e-01, 6.2273e-01, 5.2618e-01],\n",
       "         [2.7184e-01, 8.2040e-01, 7.0505e-01, 1.9547e-03, 9.7553e-01,\n",
       "          8.3641e-01, 3.5369e-01, 8.9235e-01, 9.6713e-01, 2.7904e-01,\n",
       "          6.5727e-01, 4.6682e-02, 7.3084e-01, 2.9713e-01, 1.7816e-01,\n",
       "          6.6351e-01, 9.2543e-01, 7.1112e-01, 7.0237e-01, 3.6901e-01,\n",
       "          9.8627e-01, 1.0737e-01, 1.8575e-01, 7.1964e-01, 8.4966e-01,\n",
       "          3.7092e-01, 4.7023e-01, 1.8327e-01],\n",
       "         [9.2040e-01, 3.4815e-01, 3.6748e-01, 8.4461e-01, 6.5599e-01,\n",
       "          9.3018e-01, 3.4145e-01, 6.6445e-01, 5.8840e-01, 5.7527e-01,\n",
       "          5.1490e-01, 3.9917e-01, 5.1881e-01, 9.3470e-01, 8.7243e-01,\n",
       "          7.8768e-02, 1.4898e-01, 4.6268e-02, 4.1418e-01, 7.8822e-02,\n",
       "          6.1616e-01, 5.5238e-01, 7.7748e-01, 4.2238e-01, 6.1949e-01,\n",
       "          8.7915e-01, 9.5405e-01, 2.2672e-01],\n",
       "         [1.7978e-01, 6.1601e-01, 2.1531e-01, 9.5322e-01, 9.1987e-01,\n",
       "          4.9510e-02, 3.6713e-03, 1.8724e-01, 9.1109e-02, 6.6426e-01,\n",
       "          5.1543e-01, 1.5209e-01, 8.9404e-01, 6.6253e-01, 9.7172e-01,\n",
       "          5.6319e-01, 9.5862e-01, 8.5453e-01, 4.9166e-01, 9.3265e-01,\n",
       "          2.7914e-02, 6.3248e-01, 7.4069e-01, 2.3282e-01, 2.7088e-01,\n",
       "          4.6120e-01, 1.5294e-01, 5.1804e-01],\n",
       "         [1.0796e-01, 4.9421e-01, 1.4130e-01, 4.4974e-01, 3.3071e-02,\n",
       "          4.1775e-01, 6.0087e-01, 4.9785e-02, 6.3063e-01, 3.0046e-01,\n",
       "          3.2946e-01, 4.1939e-01, 5.6425e-01, 8.7196e-01, 9.8850e-01,\n",
       "          9.4036e-01, 9.0631e-01, 6.0272e-01, 3.4958e-01, 3.4591e-01,\n",
       "          8.5015e-01, 2.6045e-01, 1.8413e-01, 2.1606e-01, 3.1369e-01,\n",
       "          4.3384e-01, 7.0775e-01, 8.6505e-01],\n",
       "         [7.3705e-01, 9.0970e-01, 7.4413e-01, 9.8183e-01, 1.7398e-01,\n",
       "          5.5950e-01, 7.3221e-01, 4.9519e-01, 4.7120e-01, 6.4602e-01,\n",
       "          1.0261e-01, 3.8354e-02, 5.5940e-01, 8.7117e-01, 4.8155e-01,\n",
       "          2.7565e-01, 5.8992e-01, 1.3032e-01, 3.7135e-01, 8.1176e-01,\n",
       "          4.3909e-01, 6.8886e-01, 5.1268e-01, 3.3027e-01, 6.6767e-01,\n",
       "          4.1890e-01, 7.4399e-01, 5.3511e-01],\n",
       "         [7.1685e-02, 5.0785e-02, 2.8550e-01, 3.2980e-01, 2.9190e-01,\n",
       "          6.2668e-01, 3.9386e-01, 3.0268e-01, 1.9688e-01, 2.0668e-01,\n",
       "          6.2990e-02, 2.3620e-02, 1.5332e-01, 8.4848e-02, 7.2982e-01,\n",
       "          6.5307e-01, 9.4846e-01, 8.1925e-01, 9.6057e-01, 7.6299e-01,\n",
       "          2.3882e-01, 3.0059e-01, 3.1479e-01, 7.1004e-01, 3.5280e-01,\n",
       "          9.6470e-01, 2.4373e-01, 2.1508e-01],\n",
       "         [1.1024e-01, 3.3787e-02, 9.4641e-01, 1.9596e-01, 6.3541e-01,\n",
       "          6.8303e-01, 3.7153e-01, 3.4130e-01, 2.2840e-01, 7.6657e-01,\n",
       "          4.9706e-01, 7.2561e-02, 9.9337e-01, 3.2437e-01, 5.5732e-01,\n",
       "          1.4913e-01, 4.4492e-01, 5.3597e-01, 1.3195e-01, 3.9486e-01,\n",
       "          5.8680e-01, 9.2745e-02, 8.1649e-01, 6.9089e-01, 1.8480e-01,\n",
       "          8.6272e-02, 1.0725e-01, 2.6773e-01],\n",
       "         [4.2933e-02, 3.0829e-01, 3.5644e-01, 8.5750e-01, 3.1668e-02,\n",
       "          8.5499e-01, 4.4327e-01, 8.5629e-01, 2.8050e-01, 7.1973e-01,\n",
       "          1.4057e-01, 9.1961e-01, 2.5837e-01, 4.2142e-01, 7.6092e-01,\n",
       "          2.7836e-01, 3.9236e-01, 2.7590e-01, 8.9184e-01, 8.6777e-01,\n",
       "          1.9306e-02, 7.0768e-01, 6.5312e-02, 8.6969e-01, 1.4199e-01,\n",
       "          9.3156e-01, 7.6664e-01, 2.3199e-01]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "124b2e1e048c176ee2ffe5b73aa94387eaae78bfeef6f5c8386705ace3e56876"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('yuan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
