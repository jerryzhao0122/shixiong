{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import collections\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm,trange\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "# from torchvision.transforms.functional import InterpolationMode\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1.7.1+cu110'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN,self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84,10)\n",
    "#         self.dropout = nn.Dropout(p=0.5)  \n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 16 * 5 * 5)\n",
    "#         x = F.relu(self.fc1(self.dropout(x)))\n",
    "#         x = F.relu(self.fc2(self.dropout(x)))\n",
    "#         x = self.fc3(self.dropout(x))\n",
    "#         return F.log_softmax(x, dim=1)  \n",
    " \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(self.dropout(x)))\n",
    "        x = F.relu(self.fc2(self.dropout(x)))\n",
    "        x = F.relu(self.fc3(self.dropout(x)))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# class AlexNet(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Neural network model consisting of layers propsed by AlexNet paper.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, num_classes=2):\n",
    "#         \"\"\"\n",
    "#         Define and allocate layers for this neural net.\n",
    "#         Args:\n",
    "#             num_classes (int): number of classes to predict with this model\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         # input size should be : (b x 3 x 227 x 227)\n",
    "#         # The image in the original paper states that width and height are 224 pixels, but\n",
    "#         # the dimensions after first convolution layer do not lead to 55 x 55.\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\n",
    "#             nn.ReLU(),\n",
    "#             nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\n",
    "#             nn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\n",
    "#             nn.ReLU(),\n",
    "#             nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\n",
    "#             nn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\n",
    "#         )\n",
    "#         # classifier is just a name for linear layers\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(p=0.5, inplace=True),\n",
    "#             nn.Linear(in_features=(256 * 6 * 6), out_features=500),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=0.5, inplace=True),\n",
    "#             nn.Linear(in_features=500, out_features=20),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(in_features=20, out_features=num_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Pass the input through the net.\n",
    "#         Args:\n",
    "#             x (Tensor): input tensor\n",
    "#         Returns:\n",
    "#             output (Tensor): output tensor\n",
    "#         \"\"\"\n",
    "#         x = self.net(x)\n",
    "#         x = x.view(-1, 256 * 6 * 6)  # reduce the dimensions for linear layer input\n",
    "#         return self.classifier(x)\n",
    "\n",
    "#定义Alexnet网路结构\n",
    "class AlexNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=10, init_weights=False):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(torch.nn.Conv2d(3, 96, kernel_size=5, stride=2, padding=2),\n",
    "                                          # raw kernel_size=11, stride=4, padding=2. For use img size 224 * 224.\n",
    "                                          torch.nn.BatchNorm2d(96),\n",
    "                                          torch.nn.ReLU(inplace=True),\n",
    "                                          torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        self.layer2 = torch.nn.Sequential(torch.nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "                                          torch.nn.BatchNorm2d(256),\n",
    "                                          torch.nn.ReLU(inplace=True),\n",
    "                                          torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
    "        self.layer3 = torch.nn.Sequential(torch.nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "                                          torch.nn.BatchNorm2d(384),\n",
    "                                          torch.nn.ReLU(inplace=True))\n",
    "        self.layer4 = torch.nn.Sequential(torch.nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "                                          torch.nn.BatchNorm2d(384),\n",
    "                                          torch.nn.ReLU(inplace=True))\n",
    "        self.layer5 = torch.nn.Sequential(torch.nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "                                          torch.nn.BatchNorm2d(384),\n",
    "                                          torch.nn.ReLU(inplace=True),\n",
    "                                          torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = torch.nn.Sequential(torch.nn.Dropout(p=0.5),\n",
    "                                       torch.nn.Linear(384 * 2 * 2, 4096),\n",
    "                                       torch.nn.ReLU(inplace=True))\n",
    "        self.fc2 = torch.nn.Sequential(torch.nn.Dropout(p=0.5),\n",
    "                                       torch.nn.Linear(4096, 4096),\n",
    "                                       torch.nn.ReLU(inplace=True))\n",
    "        self.fc3 = torch.nn.Sequential(torch.nn.Dropout(p=0.5),\n",
    "                                       torch.nn.Linear(4096, num_classes))\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train len: 15 \t test len: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/300 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f64810bc2fb14645be6080fd83777d00"
      },
      "application/json": {
       "n": 0,
       "total": 300,
       "elapsed": 0.012957334518432617,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_3131071/519425542.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     89\u001B[0m     \u001B[0;31m# for e in tqdm(range(epoch)):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 91\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     92\u001B[0m             \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtarget\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m             \u001B[0mlog_probs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    433\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    434\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 435\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    436\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    437\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1066\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1067\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_shutdown\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tasks_outstanding\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1068\u001B[0;31m             \u001B[0midx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1069\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tasks_outstanding\u001B[0m \u001B[0;34m-=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1070\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1032\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1033\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1034\u001B[0;31m                 \u001B[0msuccess\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_try_get_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1035\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0msuccess\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1036\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    870\u001B[0m         \u001B[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    871\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 872\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data_queue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    873\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    874\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/multiprocessing/queues.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    111\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_rlock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelease\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m         \u001B[0;31m# unserialize the data after having released the lock\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 113\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_ForkingPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mres\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    114\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mqsize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001B[0m in \u001B[0;36mrebuild_storage_fd\u001B[0;34m(cls, df, size)\u001B[0m\n\u001B[1;32m    280\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mrebuild_storage_fd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 282\u001B[0;31m     \u001B[0mfd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    283\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    284\u001B[0m         \u001B[0mstorage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstorage_from_cache\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfd_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/multiprocessing/resource_sharer.py\u001B[0m in \u001B[0;36mdetach\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m             \u001B[0;34m'''Get the fd.  This should only be called once.'''\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m             \u001B[0;32mwith\u001B[0m \u001B[0m_resource_sharer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_connection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_id\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mconn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     58\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_handle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/multiprocessing/resource_sharer.py\u001B[0m in \u001B[0;36mget_connection\u001B[0;34m(ident)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mconnection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mClient\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0maddress\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mident\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m         \u001B[0mc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mClient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maddress\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauthkey\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mprocess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcurrent_process\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauthkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m         \u001B[0mc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetpid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36mClient\u001B[0;34m(address, family, authkey)\u001B[0m\n\u001B[1;32m    497\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mauthkey\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    498\u001B[0m         \u001B[0manswer_challenge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauthkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 499\u001B[0;31m         \u001B[0mdeliver_challenge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauthkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    500\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    501\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36mdeliver_challenge\u001B[0;34m(connection, authkey)\u001B[0m\n\u001B[1;32m    728\u001B[0m     \u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCHALLENGE\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    729\u001B[0m     \u001B[0mdigest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhmac\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnew\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mauthkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'md5'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdigest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 730\u001B[0;31m     \u001B[0mresponse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m256\u001B[0m\u001B[0;34m)\u001B[0m        \u001B[0;31m# reject large message\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    731\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mresponse\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mdigest\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    732\u001B[0m         \u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mWELCOME\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36mrecv_bytes\u001B[0;34m(self, maxlength)\u001B[0m\n\u001B[1;32m    214\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmaxlength\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mmaxlength\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"negative maxlength\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 216\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmaxlength\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    217\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mbuf\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    218\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_bad_message_length\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36m_recv_bytes\u001B[0;34m(self, maxsize)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    406\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 407\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    408\u001B[0m         \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munpack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"!i\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmaxsize\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0msize\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/cloud/ffl/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36m_recv\u001B[0;34m(self, size, read)\u001B[0m\n\u001B[1;32m    377\u001B[0m         \u001B[0mremaining\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0mremaining\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 379\u001B[0;31m             \u001B[0mchunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mremaining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    380\u001B[0m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from random import random, randint\n",
    "\n",
    "# transform = transforms.Compose([transforms.Resize((224, 224), interpolation=InterpolationMode.BICUBIC),\n",
    "#                                 transforms.ToTensor()\n",
    "#                                 ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                                ])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "            root=\"/home/featurize/data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform\n",
    "        )\n",
    "        \n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "            root=\"/home/featurize/data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform\n",
    "        )\n",
    "\n",
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "\n",
    "# print('train len: {} \\t test len: {}'.format(len(train_dataset),len(test_dataset)))\n",
    "\n",
    "samp_idxs = list(range(30000))\n",
    "test_idxs = list(range(40000,50000))\n",
    "train_dl = DataLoader(DatasetSplit(train_dataset,samp_idxs),batch_size=2048,num_workers=6)\n",
    "test_dl = DataLoader(DatasetSplit(train_dataset,test_idxs),batch_size=2048,num_workers=6)\n",
    "\n",
    "# train_dataset.data = train_dataset.data[:30000]\n",
    "# train_dataset.targets = train_dataset.targets[:3000]\n",
    "\n",
    "# train_dl = DataLoader(train_dataset,batch_size=1024,num_workers=5)\n",
    "# test_dl = DataLoader(test_dataset,batch_size=1024,num_workers=5)\n",
    "\n",
    "# model = AlexNet().to(device)\n",
    "model = CNN().to(device)\n",
    "\n",
    "print('train len: {} \\t test len: {}'.format(len(train_dl),len(test_dl)))\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=0.1,momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.2)\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "def inference(model,dataloader):\n",
    "        model = model\n",
    "        model.eval()\n",
    "        loss,total,correct = 0.0,0.0,0.0        \n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Inference\n",
    "            outputs = model(images)\n",
    "            # print(outputs)\n",
    "            # print(labels)\n",
    "            batch_loss = criterion(outputs, labels)\n",
    "            loss += float(batch_loss.item())\n",
    "\n",
    "            # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "            total += len(labels)\n",
    "\n",
    "        accuracy = correct/total\n",
    "        return accuracy, loss\n",
    "\n",
    "epoch = 301\n",
    "with trange(1,epoch) as t:\n",
    "    for e in t:\n",
    "    # for e in tqdm(range(epoch)):\n",
    "        model.train()\n",
    "        for batch_idx,(data,target) in enumerate(train_dl):\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "            log_probs = model(data)\n",
    "            loss = criterion(log_probs,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        acc,loss = inference(model,test_dl)\n",
    "        t.set_postfix(epoch=e,acc=acc,loss=loss)\n",
    "\n",
    "        if e in [100,150,200,250,300]:\n",
    "            path = '/home/featurize/result/models/white_box_target_test_lanet'+str(e)+'.pth.tar'\n",
    "            torch.save(model.state_dict(),path)\n",
    "            print('已经保存')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "0\n",
      "forward layer1.0 torch.Size([1, 96, 16, 16])\n",
      "forward layer2.0 torch.Size([1, 256, 8, 8])\n",
      "forward layer3.0 torch.Size([1, 384, 4, 4])\n",
      "forward layer4.0 torch.Size([1, 384, 4, 4])\n",
      "forward layer5.0 torch.Size([1, 384, 4, 4])\n",
      "forward fc1.1 torch.Size([1, 4096])\n",
      "forward fc2.1 torch.Size([1, 4096])\n",
      "forward fc3.1 torch.Size([1, 10])\n",
      "backwoard fc3.1 torch.Size([4096, 10]) torch.Size([1, 4096]) torch.Size([1, 10])\n",
      "backwoard fc2.1 torch.Size([4096, 4096]) torch.Size([1, 4096]) torch.Size([1, 4096])\n",
      "backwoard fc1.1 torch.Size([1536, 4096]) torch.Size([1, 1536]) torch.Size([1, 4096])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_195290/3117146838.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 60\u001B[0;31m     \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     61\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;31m# for name, layer in model.named_modules():\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    306\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 307\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    308\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    154\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 156\u001B[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    157\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_195290/3117146838.py\u001B[0m in \u001B[0;36mbhook\u001B[0;34m(model, input, output)\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mbhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[0mgrad_layer_features\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'backwoard'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mbhook\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "selected_conv_layer_names = ['layer1.0','layer2.0','layer3.0','layer4.0','layer5.0',\n",
    "                            'fc1.1','fc2.1','fc3.1']\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AlexNet().to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                                ])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "            root=\"/home/featurize/data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform\n",
    "        )\n",
    "\n",
    "train_dl = DataLoader(train_dataset,batch_size=1,num_workers=6)\n",
    "\n",
    "\n",
    "hidden_layer_features = {}\n",
    "def get_hidden_layer_features(name):\n",
    "    def fhook(model, input, output):\n",
    "        hidden_layer_features[name] = output\n",
    "        print('forward',name,output.shape)\n",
    "    return fhook\n",
    "\n",
    "grad_layer_features = {}\n",
    "def get_grad_lalyer_features(name):\n",
    "    def bhook(model,input,output):\n",
    "        grad_layer_features[name] = output\n",
    "        print('backwoard',name,input[2].shape,input[1].shape,output[0].shape)\n",
    "    return bhook\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=0.02,momentum=0.5)\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(train_dl):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(batch_idx)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    for name, layer in model.named_modules():\n",
    "        if name in selected_conv_layer_names:\n",
    "            if isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "                layer.register_forward_hook(get_hidden_layer_features(name))\n",
    "                layer.register_backward_hook(get_grad_lalyer_features(name))\n",
    "            if isinstance(layer,torch.nn.modules.linear.Linear):\n",
    "                layer.register_forward_hook(get_hidden_layer_features(name))\n",
    "                layer.register_backward_hook(get_grad_lalyer_features(name))\n",
    "    \n",
    "    output = model(images)\n",
    "\n",
    "    loss = criterion(output,labels)\n",
    "    loss.backward()\n",
    "\n",
    "    # for name, layer in model.named_modules():\n",
    "    #     if name in selected_conv_layer_names:\n",
    "    #         # if isinstance(layer, torch.nn.modules.conv.Conv2d):\n",
    "    #         #     grad_layer_features[name] = layer.weight.grad\n",
    "    #         if isinstance(layer,torch.nn.modules.linear.Linear):\n",
    "    #             # grad_layer_features[name] = layer.weight.grad\n",
    "    #             # print('backward',name,grad_layer_features[name].shape)\n",
    "    #             print(name,layer.weight.grad)\n",
    "\n",
    "    # if batch_idx==2:\n",
    "    # print(hidden_layer_features)\n",
    "    # print(grad_layer_features)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/featurize/result/models/server/iid_P_FedAvg_250.pth.tar'\n",
    "model = AlexNet().to(device)\n",
    "weight = torch.load(model_path,map_location=device)\n",
    "model.load_state_dict(weight)\n",
    "\n",
    "print(weight['layer1.0.weight'])\n",
    "\n",
    "op = torch.optim.SGD(model.parameters(),lr=1)\n",
    "cr = nn.CrossEntropyLoss()\n",
    "\n",
    "out = model(torch.rand(1,3,32,32).cuda())\n",
    "loss = cr(out,torch.tensor(1).unsqueeze(dim=0).cuda())\n",
    "loss.backward()\n",
    "op.step()\n",
    "\n",
    "print(weight['layer1.0.weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnForCnnLayer(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super().__init__()\n",
    "        self.dim1 = input_shape[0]\n",
    "        self.dim2 = input_shape[1]\n",
    "        self.dim3 = input_shape[2]\n",
    "\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.dim1,out_channels=100,kernel_size=(self.dim2,self.dim3),stride=(1,1))\n",
    "        self.fc1 = nn.Linear(100,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.conv1(self.dropout(x)))\n",
    "        print(x.shape)\n",
    "        x = torch.flatten(x,start_dim=1,end_dim=-1)\n",
    "        print(x.shape)\n",
    "        x = F.leaky_relu(self.fc1(self.dropout(x)))\n",
    "        x = F.leaky_relu(self.fc2(self.dropout(x)))\n",
    "        return x\n",
    "\n",
    "class FcnForModel(nn.Module):\n",
    "    def __init__(self, dim_in=128, dim_out=64):\n",
    "        super().__init__() \n",
    "        self.fc1 = nn.Linear(dim_in, 128)\n",
    "        self.fc2 = nn.Linear(128, dim_out)\n",
    "        self.dropout = nn.Dropout(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(self.dropout(x)))\n",
    "        x = F.leaky_relu(self.fc2(self.dropout(x)))\n",
    "        return x\n",
    "\n",
    "input_shape=(6,28,28)\n",
    "x = torch.rand(2,1)\n",
    "model = FcnForModel(dim_in=1)\n",
    "y = model(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(64,5,5)\n",
    "b = torch.rand(64,5,5)\n",
    "c= []\n",
    "c.append(a)\n",
    "c.append(b)\n",
    "\n",
    "d = torch.cat(c)\n",
    "\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b114295533213be714c497b6c7c7c36862ca698da8b4418201631177dea05d47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
